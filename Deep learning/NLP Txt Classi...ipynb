{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0ae57ed2-f326-4895-a154-62f52956265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec89eea-8ee3-459a-95b8-642ff6823bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887a6bfc-7b20-4471-87f5-de9dc0b1ccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.4-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.32.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.27.2-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.0-cp312-cp312-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Downloading spacy-3.8.4-cp312-cp312-win_amd64.whl (11.8 MB)\n",
      "   ---------------------------------------- 0.0/11.8 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 3.1/11.8 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.3/11.8 MB 18.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.8 MB 19.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.8/11.8 MB 17.2 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl (122 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 13.8 MB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 632.6/632.6 kB 7.9 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.4-cp312-cp312-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 15.3 MB/s eta 0:00:00\n",
      "Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-1.2.0-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 4.2/6.3 MB 19.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 18.3 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 23.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 19.3 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl (150 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, shellingham, pydantic-core, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, annotated-types, srsly, pydantic, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed annotated-types-0.7.0 blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 pydantic-2.10.6 pydantic-core-2.27.2 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.1 wasabi-1.1.3 weasel-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f94128-4fcb-4129-8fc8-9299b65ec6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 2.4/12.8 MB 14.9 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 6.3/12.8 MB 16.1 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.2/12.8 MB 17.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 16.0 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "313ebefd-250f-40fe-9114-d279c70ce5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "76ff9ccc-6860-4e94-b9ab-d0f53544bc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a python developer currentlyworking for a London-based Fintechcompany.\n",
      "I am interested in learningNatural Language Processing.\n"
     ]
    }
   ],
   "source": [
    "about_text=('I am a python developer currently'\n",
    "        'working for a London-based Fintech'\n",
    "        'company.I am interested in learning'\n",
    "            'Natural Language Processing.')\n",
    "about_doc=nlp(about_text)\n",
    "sentences=list(about_doc.sents)\n",
    "len(sentences)\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02662c31-6818-43d2-bc60-2b83b9b37503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3f90e832-a4fa-49eb-a94a-aab8c117ff62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 0\n",
      "am 2\n",
      "a 5\n",
      "python 7\n",
      "developer 14\n",
      "currentlyworking 24\n",
      "for 41\n",
      "a 45\n",
      "London 47\n",
      "- 53\n",
      "based 54\n",
      "Fintechcompany 60\n",
      ". 74\n",
      "I 75\n",
      "am 77\n",
      "interested 80\n",
      "in 91\n",
      "learningNatural 94\n",
      "Language 110\n",
      "Processing 119\n",
      ". 129\n"
     ]
    }
   ],
   "source": [
    "for token in about_doc:\n",
    "    print(token,token.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd538781-c159-470d-a7bc-7b5846fff741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "66ac1525-9b4a-4a3c-a154-bb0cc915c650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'a', 'python', 'developer', 'currentlyworking', 'for', 'a', 'London', '-', 'based', 'Fintechcompany.I', 'am', 'interested', 'in', 'learningNatural', 'Language', 'Processing', '.']\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "custom_nlp=spacy.load('en_core_web_sm')\n",
    "prefix_re=spacy.util.compile_prefix_regex(custom_nlp.Defaults.prefixes)\n",
    "suffix_re=spacy.util.compile_suffix_regex(custom_nlp.Defaults.suffixes)\n",
    "infix_re=re.compile(r'''[-~]''')\n",
    "def customize_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab,prefix_search=prefix_re.search,\n",
    "                     suffix_search=suffix_re.search,\n",
    "                     infix_finditer=infix_re.finditer,\n",
    "                     token_match=None\n",
    "                    )\n",
    "custom_nlp.tokenizer=customize_tokenizer(custom_nlp)\n",
    "cutom_tokenizer_about_doc=custom_nlp(about_text)\n",
    "print([token.text for token in cutom_tokenizer_about_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cb6df7-9097-4e99-963c-9b4fcf849200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "38b89278-f6b5-4d4c-85e1-f8f19f637c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "developer\n",
      "currentlyworking\n",
      "London\n",
      "-\n",
      "based\n",
      "Fintechcompany\n",
      ".\n",
      "interested\n",
      "learningNatural\n",
      "Language\n",
      "Processing\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in about_doc:\n",
    "    if not token.is_stop:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e9eccc-27c7-4fbf-afbd-2d68d4d76f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3f001ea9-7f31-41b5-b4db-1dfed40952a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raj raj\n",
      "is be\n",
      "helping helping\n",
      "organize organize\n",
      "a a\n",
      "developerconference developerconference\n",
      "on on\n",
      "Applications Applications\n",
      "of of\n",
      "Natural Natural\n",
      "Language Language\n",
      "Processing processing\n",
      ". .\n",
      "He he\n",
      "keeps keep\n",
      "organizing organize\n",
      "local local\n",
      "Python Python\n",
      "meetups meetup\n",
      "and and\n",
      "several several\n",
      "internal internal\n",
      "talks talk\n",
      "at at\n",
      "his his\n",
      "workplace workplace\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "con_help_txt=('Raj is helping organize a developer'\n",
    "\n",
    "     'conference on Applications of Natural Language'\n",
    "\n",
    "     ' Processing. He keeps organizing local Python meetups'\n",
    "\n",
    "     ' and several internal talks at his workplace.')\n",
    "con_help_doc=nlp(con_help_txt)\n",
    "for token in con_help_doc:\n",
    "    print(token,token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76de2727-42a8-488d-81e7-e2474df504fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a7620ffd-d6b1-4257-8252-a9994a8917cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('J', 4), ('London', 3), ('Natural', 3), ('Language', 3), ('Processing', 3)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "complete_txt=('J K is a Python developer currently'\n",
    "\n",
    "    'working for a London-based Fintech company. He is'\n",
    "\n",
    "    ' interested in learning Natural Language Processing.'\n",
    "\n",
    "    ' There is a conference happening on 21 June'\n",
    "\n",
    "     ' 2019 in London. It is titled \"Applications of Natural'\n",
    "\n",
    "    ' Language Processing\". There is a helpline number '\n",
    "\n",
    "    ' available . J is helping organize it.'\n",
    "\n",
    "    ' He keeps organizing local Python meetups and several'\n",
    "\n",
    "    ' internal talks at his workplace. J is also presenting'\n",
    "\n",
    "     ' a talk. The talk will introduce the reader about \"Use'\n",
    "\n",
    "     ' cases of Natural Language Processing in Fintech\".'\n",
    "\n",
    "     ' Apart from his work, he is very passionate about music.'\n",
    "\n",
    "     ' J is learning to play the Piano. He has enrolled '\n",
    "\n",
    "     ' himself in the weekend batch of Great Piano Academy.'\n",
    "\n",
    "     ' Great Piano Academy is situated in Mayfair or the City'\n",
    "\n",
    "\n",
    "     ' of London and has world-class piano instructors.')\n",
    "\n",
    "complete_doc=nlp(complete_txt)\n",
    "words=[token.text for token in complete_doc\n",
    "       if not token.is_stop and not token.is_punct]\n",
    "word_freq=Counter(words)\n",
    "common_words=word_freq.most_common(5)\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a37f745-8231-4a51-993f-7d71e43f79fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f87f3540-91d0-49cf-8f7d-7db28976f478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['K', 'developer', 'currentlyworking', 'based', 'company', 'interested', 'conference', 'happening', '21', 'June', '2019', 'titled', 'Applications', 'helpline', 'number', 'available', 'helping', 'organize', 'keeps', 'organizing', 'local', 'meetups', 'internal', 'talks', 'workplace', 'presenting', 'introduce', 'reader', 'Use', 'cases', 'Apart', 'work', 'passionate', 'music', 'play', 'enrolled', 'weekend', 'batch', 'situated', 'Mayfair', 'City', 'world', 'class', 'piano', 'instructors']\n"
     ]
    }
   ],
   "source": [
    "unique_words=[word for(word,freq)in word_freq.items()if freq==1]\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a722d06e-009f-47dc-a47a-99d5cd2f3487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3b81b517-7922-435a-a8ee-53a41b13fba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piano Academy 0 13 ORG Companies, agencies, institutions, etc.\n",
      "Mayfaire 28 36 ORG Companies, agencies, institutions, etc.\n",
      "London 52 58 GPE Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "piano_txt=('Piano Academy is situated'\n",
    "           'in Mayfaire or the city of London and has'\n",
    "           'world-class piano instructors.')\n",
    "piano_doc=nlp(piano_txt)\n",
    "for ent in piano_doc.ents:\n",
    "    print(ent.text,ent.start_char,ent.end_char,\n",
    "          ent.label_,spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c00fb-8100-47ec-8f68-80aa07b682b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "84743b2c-a5a6-48d3-b498-7f6dc886022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2f58f618-1629-49cf-9f23-1b174a93ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_a='this document is first document'\n",
    "doc_b='this document is the second document'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6b66216c-4f2f-40b3-b3dd-3697a83174cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_a=doc_a.split(' ')\n",
    "bag_of_words_b=doc_b.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4d2f5e84-8517-4f75-a23e-8c553e432a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is', 'first', 'document', 'second', 'this', 'the'}\n"
     ]
    }
   ],
   "source": [
    "unique_words_set=set(bag_of_words_a).union(set(bag_of_words_b))\n",
    "print(unique_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ce0ee336-b340-46d1-bac5-1d0e634c8de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 0, 'first': 0, 'document': 0, 'second': 0, 'this': 0, 'the': 0}\n"
     ]
    }
   ],
   "source": [
    "dict_a=dict.fromkeys(unique_words_set,0)\n",
    "print(dict_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b66d9e80-2470-4355-9f2e-577264bfa3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 1, 'first': 1, 'document': 2, 'second': 0, 'this': 1, 'the': 0}\n"
     ]
    }
   ],
   "source": [
    "for word in bag_of_words_a:\n",
    "    dict_a[word]+=1\n",
    "print(dict_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "46f81ab4-d9f5-4a6f-b5e8-2dd3239b7763",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_b=dict.fromkeys(unique_words_set,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f51bfa14-edda-4356-9668-72710d7e6fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 1, 'first': 0, 'document': 2, 'second': 1, 'this': 1, 'the': 1}\n"
     ]
    }
   ],
   "source": [
    "for word in bag_of_words_b:\n",
    "    dict_b[word]+=1\n",
    "print(dict_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad1db62-51cc-46f7-a9b1-753c34b4e840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c5df3b28-ef57-45b3-942b-cb7a423484d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 0.2}\n"
     ]
    }
   ],
   "source": [
    "def compute_term_frequency(word_dictionary,bag_of_words):\n",
    "    term_frequency_dictionary={}\n",
    "    length_of_bag_of_words=len(bag_of_words)\n",
    "    for word,count in word_dictionary.items():\n",
    "        term_frequency_dictionary[word]=count/float(length_of_bag_of_words)\n",
    "        return term_frequency_dictionary\n",
    "print(compute_term_frequency(dict_a,bag_of_words_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4809f41-fc9e-42b0-9ac1-759ada8c1309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8521bc5a-9510-4a6c-949b-8be6aeafdef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def compute_inverse_document_frequency(full_doc_list):\n",
    "    idf_dic={}\n",
    "    length_of_doc_list=len(full_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1df1a90b-95a2-4148-8a47-9c1917a2a4d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_doc_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m idf_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mfromkeys(\u001b[43mfull_doc_list\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys(),\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word,value \u001b[38;5;129;01min\u001b[39;00m idf_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      3\u001b[0m     idf_dict[word]\u001b[38;5;241m=\u001b[39mmath\u001b[38;5;241m.\u001b[39mlog(length_of_doc_list\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(value)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'full_doc_list' is not defined"
     ]
    }
   ],
   "source": [
    "idf_dict=dict.fromkeys(full_doc_list[0].keys(),0)\n",
    "for word,value in idf_dict.items():\n",
    "    idf_dict[word]=math.log(length_of_doc_list/(float(value)+1))\n",
    "return idf_dict\n",
    "final_idf_dict=compute_inverse_document_frequency([dict_a,dict_b])\n",
    "print(final_idf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b88beeee-e57b-430f-b894-b936022f94ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': -0.40546510810816444, 'document': -0.40546510810816444, 'is': -0.40546510810816444, 'first': 0.0, 'the': 0.0, 'second': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def compute_inverse_document_frequency(full_doc_list):\n",
    "    length_of_doc_list = len(full_doc_list)  # Number of documents\n",
    "\n",
    "    # ✅ Initialize IDF dictionary with words from the first document\n",
    "    idf_dict = dict.fromkeys(full_doc_list[0].keys(), 0)\n",
    "\n",
    "    # ✅ Count how many documents contain each word\n",
    "    for doc in full_doc_list:\n",
    "        for word, value in doc.items():\n",
    "            if value > 0:  # The word appears in the document\n",
    "                idf_dict[word] += 1\n",
    "\n",
    "    # ✅ Compute IDF using log(N / (df + 1))\n",
    "    for word in idf_dict:\n",
    "        idf_dict[word] = math.log(length_of_doc_list / (idf_dict[word] + 1))\n",
    "\n",
    "    return idf_dict  # ✅ Correct indentation\n",
    "\n",
    "# Example dictionaries (word counts per document)\n",
    "dict_a = {'this': 1, 'document': 2, 'is': 1, 'first': 1, 'the': 0, 'second': 0}\n",
    "dict_b = {'this': 1, 'document': 2, 'is': 1, 'first': 0, 'the': 1, 'second': 1}\n",
    "\n",
    "# ✅ Pass the correctly defined full_doc_list\n",
    "full_doc_list = [dict_a, dict_b]\n",
    "\n",
    "final_idf_dict = compute_inverse_document_frequency(full_doc_list)\n",
    "print(final_idf_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d709a68e-8f4e-4814-93f8-8568fc0ac8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6ce5ca-3e83-4d9b-a91a-68a4e3fc9bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcf39ee-d0dc-4f36-8a2a-3fa7952a9727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
